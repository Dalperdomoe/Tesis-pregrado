{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torchmps import MPS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from gensim.models import word2vec\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3,\"Wall St. Bears Claw Back Into the Black (Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3,\"Carlyle Looks Toward Commercial Aerospace (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3,\"Oil and Economy Cloud Stocks' Outlook (Reut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3,\"Iraq Halts Oil Exports from Main Southern P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3,\"Oil prices soar to all-time record, posing ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  3,\"Wall St. Bears Claw Back Into the Black (Re...\n",
       "1  3,\"Carlyle Looks Toward Commercial Aerospace (...\n",
       "2  3,\"Oil and Economy Cloud Stocks' Outlook (Reut...\n",
       "3  3,\"Iraq Halts Oil Exports from Main Southern P...\n",
       "4  3,\"Oil prices soar to all-time record, posing ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=pd.read_csv(\"./DataSet/AG-NEWS/train.csv\", sep=\"\\t\" ,  header=None)\n",
    "df2=pd.read_csv(\"./DataSet/AG-NEWS/test.csv\", sep=\"\\t\" , header=None)\n",
    "\n",
    "df = pd.concat([df1,df2])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7595</td>\n",
       "      <td>1</td>\n",
       "      <td>Ukrainian presidential candidate Viktor Yushch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7596</td>\n",
       "      <td>2</td>\n",
       "      <td>With the supply of attractive pitching options...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7597</td>\n",
       "      <td>2</td>\n",
       "      <td>Like Roger Clemens did almost exactly eight ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7598</td>\n",
       "      <td>3</td>\n",
       "      <td>SINGAPORE : Doctors in the United States have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7599</td>\n",
       "      <td>3</td>\n",
       "      <td>EBay plans to buy the apartment and home renta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0                                                  2\n",
       "0     3  Reuters - Short-sellers, Wall Street's dwindli...\n",
       "1     3  Reuters - Private investment firm Carlyle Grou...\n",
       "2     3  Reuters - Soaring crude prices plus worries\\ab...\n",
       "3     3  Reuters - Authorities have halted oil export\\f...\n",
       "4     3  AFP - Tearaway world oil prices, toppling reco...\n",
       "...  ..                                                ...\n",
       "7595  1  Ukrainian presidential candidate Viktor Yushch...\n",
       "7596  2  With the supply of attractive pitching options...\n",
       "7597  2  Like Roger Clemens did almost exactly eight ye...\n",
       "7598  3  SINGAPORE : Doctors in the United States have ...\n",
       "7599  3  EBay plans to buy the apartment and home renta...\n",
       "\n",
       "[127600 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[0].str.split(pat=',\"',expand=True)\n",
    "del df[1]\n",
    "del df[3]\n",
    "del df[4]\n",
    "del df[5]\n",
    "del df[6]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    31900\n",
       "3    31900\n",
       "2    31900\n",
       "4    31900\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escoger subconjunto \n",
    "\n",
    "sample = 5000\n",
    "\n",
    "df1 = df[(df[0] == \"1\") & df[2].notnull()].sample(sample, random_state=1234)\n",
    "df2 = df[(df[0] == \"2\") & df[2].notnull()].sample(sample, random_state=1234)\n",
    "df3 = df[(df[0] == \"3\") & df[2].notnull()].sample(sample, random_state=1234)\n",
    "df4 = df[(df[0] == \"4\") & df[2].notnull()].sample(sample, random_state=1234)\n",
    "\n",
    "\n",
    "df = pd.concat([df1,df2,df3,df4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    5000\n",
       "2    5000\n",
       "1    5000\n",
       "3    5000\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df[2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0] = df[0].astype('category')\n",
    "cat_columns = df.select_dtypes(['category']).columns\n",
    "label = df[cat_columns].apply(lambda x: x.cat.codes).values.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos los tokenizer y lemmatizer\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "lem = nltk.WordNetLemmatizer()\n",
    "# Descargamos las stopwords para inglés\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "# Definimos la función de preprocesamiento\n",
    "def normalize_document(doc):\n",
    "    # Se eliminan caracteres especiales\n",
    "    doc = re.sub(r'\\n', '', doc)\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    # Se convierten los téxtos a minúsculas\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # Tokenizado de documento\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # Eliminación de stopwords\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Lematización\n",
    "    tokens = [lem.lemmatize(token) for token in tokens]\n",
    "    # Retornamos una versión filtrada del texto\n",
    "    doc = ' '.join(tokens)\n",
    "    return doc\n",
    "# Vectorización de la función\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "norm_corpus = normalize_corpus(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representacion word2vec\n",
    "\n",
    "Los siguientes parámetros son utilizados por el modelo Word2vec para construir el modelo:\n",
    "\n",
    "* feature_size: Determina la dimensión de los vectores de embedding.\n",
    "* window_context: Es el número de palabras en el vecindario que constituye el contexto.\n",
    "* min_word_count: Especifica el conteo mínimo de una palabra dentro del corpus para ser incluida dentro del vocabulario.\n",
    "* sample: este parámetro es usado para el sub-muestreo dentro del algoritmo. Generalmente, los valores entre 0.01 entre 0.0001 funcionan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_size = 100 \n",
    "window_context = 10\n",
    "min_word_count = 1 \n",
    "sample = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "\n",
    "tokenized_corpus = [wpt.tokenize(document) for document in norm_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = word2vec.Word2Vec(tokenized_corpus, size=feature_size,\n",
    "                              window=window_context, min_count=min_word_count,\n",
    "                              sample=sample, iter=100)\n",
    "\n",
    "w2v_representations=[]\n",
    "for i,sentence in enumerate(tokenized_corpus):\n",
    "    try:\n",
    "        # La representación de un documento es el promedio de la representación\n",
    "        # de cada uno de sus términos.\n",
    "        w2v_representations.append(w2v_model.wv[sentence].mean(axis=0))\n",
    "    except:\n",
    "        # Hay algunos casos que sólo contenían stopwords o caracteres especiales\n",
    "        # Como no tienen representación vectorial asignamos un vector de zeros.\n",
    "        w2v_representations.append(np.zeros(shape=(feature_size,)))\n",
    "\n",
    "X = np.array(w2v_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14000, 100) (14000,)\n",
      "(6000, 100) (6000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, label, test_size=0.3, random_state=42, stratify = label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformar a tensor de torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.Tensor(X_train)\n",
    "x_test = torch.Tensor(X_test)\n",
    "y_train = torch.Tensor(Y_train).type(torch.long)\n",
    "y_test = torch.Tensor(Y_test).type(torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crear conjunto de datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "test_set = torch.utils.data.TensorDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializacion\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Parametros de MPS\n",
    "bond_dim = [8]\n",
    "rate = [1e-6,1e-7]\n",
    "\n",
    "for bond in bond_dim:\n",
    "    \n",
    "    for learn_rate in rate:\n",
    "    \n",
    "        # Parametros de entrenamiento\n",
    "        num_train  = len(train_set.tensors[0])\n",
    "        num_test   = len(test_set.tensors[0])\n",
    "        batch_size = 50\n",
    "        num_epochs = 100\n",
    "        l2_reg     = 0.\n",
    "\n",
    "        # Inicializar el modulo MPS\n",
    "        mps = MPS(input_dim=100, output_dim=4, bond_dim=bond, parallel_eval=True)\n",
    "\n",
    "\n",
    "        # Implementar mapeo de caracteristicas\n",
    "        def feature_map(data):\n",
    "            return torch.Tensor([1.0, data])\n",
    "\n",
    "        mps.register_feature_map(feature_map)\n",
    "\n",
    "        # Establecer funcion de perdida y optimizador\n",
    "        loss_fun = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(mps.parameters(), lr=learn_rate, weight_decay=l2_reg)\n",
    "\n",
    "        samplers = {'train': torch.utils.data.SubsetRandomSampler(range(num_train)),\n",
    "                    'test': torch.utils.data.SubsetRandomSampler(range(num_test))}\n",
    "\n",
    "        loaders = {name: torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                   sampler=samplers[name], drop_last=True) for (name, dataset) in \n",
    "                   [('train', train_set), ('test', test_set)]}\n",
    "\n",
    "        num_batches = {name: total_num // batch_size for (name, total_num) in\n",
    "                       [('train', num_train), ('test', num_test)]}\n",
    "\n",
    "        cfun = []\n",
    "        cfun_test = []\n",
    "        err_train = []\n",
    "        err_test = []\n",
    "\n",
    "        # Empecemos a entrenar\n",
    "        for epoch_num in range(1, num_epochs+1):\n",
    "            running_loss = 0.\n",
    "            running_acc = 0.\n",
    "\n",
    "            for inputs, labels in loaders['train']: # obtener Batch\n",
    "                inputs, labels = inputs.view([batch_size, 100]), labels.data\n",
    "\n",
    "                # Llamar nuestro MPS para obtener puntaje logit y predicciones\n",
    "                scores = mps(inputs) # Pasar Batch\n",
    "                _, preds = torch.max(scores, 1)\n",
    "\n",
    "                # Calcule la perdida y la precision, Añadalos a los running totales\n",
    "                loss = loss_fun(scores, labels) # calcular loss\n",
    "                with torch.no_grad():\n",
    "                    accuracy = torch.sum(preds == labels).item() / batch_size\n",
    "                    running_loss += loss\n",
    "                    running_acc += accuracy\n",
    "\n",
    "                # Backpropagate y actualizar parametros\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward() # Calcular gradientes\n",
    "                optimizer.step() # Actualizar pesos\n",
    "\n",
    "            cfun.append((running_loss / num_batches['train']).item())\n",
    "            err_train.append(running_acc / num_batches['train'])\n",
    "\n",
    "            # Evaluar precision sobre clasificador MPS sobre el conjunto de prueba\n",
    "            with torch.no_grad():\n",
    "                running_loss = 0.\n",
    "                running_acc = 0.\n",
    "\n",
    "                for inputs, labels in loaders['test']:\n",
    "                    inputs, labels = inputs.view([batch_size, 100]), labels.data\n",
    "\n",
    "                    # Llamar nuestro MPS para obtener puntaje logit y predicciones\n",
    "                    scores = mps(inputs)\n",
    "                    _, preds = torch.max(scores, 1)\n",
    "\n",
    "                    running_loss += loss_fun(scores, labels) # calcular loss\n",
    "                    running_acc += torch.sum(preds == labels).item() / batch_size\n",
    "\n",
    "            cfun_test.append((running_loss / num_batches['train']).item())\n",
    "            err_test.append(running_acc / num_batches['test'])\n",
    "\n",
    "\n",
    "        with open(f'./Data/TEST-rate{learn_rate}-BOND{bond}AGNEWS.txt', 'w') as f:\n",
    "            for item in err_test:\n",
    "                f.write(\"%s\\n\" % item)\n",
    "\n",
    "        with open(f'./Data/TRAIN-rate{learn_rate}-BOND{bond}AGNEWS.txt', 'w') as f:\n",
    "            for item in err_train:\n",
    "                f.write(\"%s\\n\" % item)\n",
    "\n",
    "        with open(f'./Data/LOSS-rate{learn_rate}-BOND{bond}AGNEWS.txt', 'w') as f:\n",
    "            for item in cfun:\n",
    "                f.write(\"%s\\n\" % item)\n",
    "\n",
    "        with open(f'./Data/LOSS-TEST-rate{learn_rate}-BOND{bond}AGNEWS.txt', 'w') as f:\n",
    "            for item in cfun_test:\n",
    "                f.write(\"%s\\n\" % item)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
